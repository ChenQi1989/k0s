{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"FAQ/","text":"Frequently asked questions # How is k0s promounced? # kay-zero-ess How do I run a single node cluster? # k0s server --enable-worker How do I connect to the cluster? # You find the config in /var/lib/k0s/pki/admin.conf . Copy this and change the localhost to the public ip of the controller. Use the modified config to connect with kubectl: export KUBECONFIG=/path/to/admin.conf kubectl ... Why does not kubectl get nodes show the server? # The control plane does not run kubelet at all by default, and will not accept any workloads, so the server will not show up on the node list in kubectl. If you want your server accept work loads you do so with: k0s server --enable-worker","title":"Frequently asked questions"},{"location":"FAQ/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"FAQ/#how-is-k0s-promounced","text":"kay-zero-ess","title":"How is k0s promounced?"},{"location":"FAQ/#how-do-i-run-a-single-node-cluster","text":"k0s server --enable-worker","title":"How do I run a single node cluster?"},{"location":"FAQ/#how-do-i-connect-to-the-cluster","text":"You find the config in /var/lib/k0s/pki/admin.conf . Copy this and change the localhost to the public ip of the controller. Use the modified config to connect with kubectl: export KUBECONFIG=/path/to/admin.conf kubectl ...","title":"How do I connect to the cluster?"},{"location":"FAQ/#why-does-not-kubectl-get-nodes-show-the-server","text":"The control plane does not run kubelet at all by default, and will not accept any workloads, so the server will not show up on the node list in kubectl. If you want your server accept work loads you do so with: k0s server --enable-worker","title":"Why does not kubectl get nodes show the server?"},{"location":"architecture/","text":"Architecture # Note: As with any young project, things change rapidly. Thus all the details in this architecture documentation may not be always up-to-date, but the high level concepts and patterns should still apply. Packaging # k0s is packaged as single, self-extracting binary which embeds Kubernetes binaries. This has many benefits: - Everything can be, and is, statically compiled - No OS level deps - No RPMs, dep's, snaps or any other OS specific packaging needed. Single \"package\" for all OSes - We can fully control the versions of each and every dependency Control plane # k0s as a single binary acts as the process supervisor for all other control plane components. This means there's no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes. k0s creates, manages and configures each of the components. k0s runs all control plane components as \"naked\" processes. So on the controller node there's no container engine running. Storage # Typically Kubernetes control plane supports only etcd as the datastore. In addition to etcd, k0s supports many other datastore options. This is achieved by including kine . Kine allows wide variety of backend data stores to be used such as MySQL, PostgreSQL, SQLite and dqlite. See more in storage documentation In case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. This means for example that by joining a new controller node with k0s server \"long-join-token\" k0s will automatically adjust the etcd cluster membership info to allow the new member to join the cluster. Note: Currently k0s cannot shrink the etcd cluster. For now user needs to manually remove the etcd member and only after that shutdown the k0s controller on the removed node. Worker plane # Like for the control plane, k0s creates and manages the core worker components as naked processes on the worker node. Currently we support only containerd as the container engine.","title":"Architecture"},{"location":"architecture/#architecture","text":"Note: As with any young project, things change rapidly. Thus all the details in this architecture documentation may not be always up-to-date, but the high level concepts and patterns should still apply.","title":"Architecture"},{"location":"architecture/#packaging","text":"k0s is packaged as single, self-extracting binary which embeds Kubernetes binaries. This has many benefits: - Everything can be, and is, statically compiled - No OS level deps - No RPMs, dep's, snaps or any other OS specific packaging needed. Single \"package\" for all OSes - We can fully control the versions of each and every dependency","title":"Packaging"},{"location":"architecture/#control-plane","text":"k0s as a single binary acts as the process supervisor for all other control plane components. This means there's no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes. k0s creates, manages and configures each of the components. k0s runs all control plane components as \"naked\" processes. So on the controller node there's no container engine running.","title":"Control plane"},{"location":"architecture/#storage","text":"Typically Kubernetes control plane supports only etcd as the datastore. In addition to etcd, k0s supports many other datastore options. This is achieved by including kine . Kine allows wide variety of backend data stores to be used such as MySQL, PostgreSQL, SQLite and dqlite. See more in storage documentation In case of k0s managed etcd, k0s manages the full lifecycle of the etcd cluster. This means for example that by joining a new controller node with k0s server \"long-join-token\" k0s will automatically adjust the etcd cluster membership info to allow the new member to join the cluster. Note: Currently k0s cannot shrink the etcd cluster. For now user needs to manually remove the etcd member and only after that shutdown the k0s controller on the removed node.","title":"Storage"},{"location":"architecture/#worker-plane","text":"Like for the control plane, k0s creates and manages the core worker components as naked processes on the worker node. Currently we support only containerd as the container engine.","title":"Worker plane"},{"location":"cloud-providers/","text":"Using cloud providers # k0s builds Kubernetes components in \"providerless\" mode. This means that there is no cloud providers built into k0s managed Kubernetes components. This means the cloud providers have to be configured \"externally\". The following steps outline how to enable cloud providers support in your k0s cluster. For more information on running Kubernetes with cloud providers see the official documentation . Enabling cloud provider support in kubelet # Even when all components are built with \"providerless\" mode, we need to be able to enable cloud provider \"mode\" for kubelet. This is done by running the workers with --enable-cloud-provider=true . This enables --cloud-provider=external on kubelet process. Deploying the actual cloud provider # From Kubernetes point of view, it does not realy matter how and where the cloud providers controller(s) are running. Of course the easiest way is to deploy them on the cluster itself. To deploy your cloud provider as k0s managed stack you can use the built-in manifest deployer . Simply drop all the needed manifests under e.g. /var/lib/k0s/manifests/aws/ directory and k0s will deploy everything. Some cloud providers do need some configuration files to be present on all the nodes or some other pre-requisites. Consult your cloud providers documentation for needed steps.","title":"Using cloud providers"},{"location":"cloud-providers/#using-cloud-providers","text":"k0s builds Kubernetes components in \"providerless\" mode. This means that there is no cloud providers built into k0s managed Kubernetes components. This means the cloud providers have to be configured \"externally\". The following steps outline how to enable cloud providers support in your k0s cluster. For more information on running Kubernetes with cloud providers see the official documentation .","title":"Using cloud providers"},{"location":"cloud-providers/#enabling-cloud-provider-support-in-kubelet","text":"Even when all components are built with \"providerless\" mode, we need to be able to enable cloud provider \"mode\" for kubelet. This is done by running the workers with --enable-cloud-provider=true . This enables --cloud-provider=external on kubelet process.","title":"Enabling cloud provider support in kubelet"},{"location":"cloud-providers/#deploying-the-actual-cloud-provider","text":"From Kubernetes point of view, it does not realy matter how and where the cloud providers controller(s) are running. Of course the easiest way is to deploy them on the cluster itself. To deploy your cloud provider as k0s managed stack you can use the built-in manifest deployer . Simply drop all the needed manifests under e.g. /var/lib/k0s/manifests/aws/ directory and k0s will deploy everything. Some cloud providers do need some configuration files to be present on all the nodes or some other pre-requisites. Consult your cloud providers documentation for needed steps.","title":"Deploying the actual cloud provider"},{"location":"configuration/","text":"k0s configuration # Control plane # k0s Control plane can be configured via a YAML config file. By default k0s server command reads a file called k0s.yaml but can be told to read any yaml file via --config option. An example config file with the most common options users should configure: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.106 sans : - my-k0s-control.my-domain.com network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 extensions : helm : repositories : - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" namespace : default spec.api # address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate spec.network # podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services. extensions.helm # List of Helm repositories and charts to deploy during cluster bootstrap. This example configures Prometheus from \"stable\" Helms chart repository. Configuring multi-node controlplane # When configuring an elastic/HA controlplane one must use same configuration options on each node for the cluster level options. Following options need to match on each node, otherwise the control plane components will end up in very unknown states: - network - storage : Needless to say, one cannot create a clustered controlplane with each node only storing data locally on SQLite. Full config reference # Note: Many of the options configure things deep down in the \"stack\" on various components. So please make sure you understand what is being configured and whether or not it works in your specific environment. A full config file with defaults generated by the k0s default-config command: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.106 sans : - 192.168.68.106 - 192.168.68.106 extraArgs : {} controllerManager : extraArgs : {} scheduler : extraArgs : {} storage : type : etcd etcd : peerAddress : 192.168.68.106 network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 provider : calico calico : mode : vxlan vxlanPort : 4789 vxlanVNI : 4096 mtu : 1450 wireguard : false podSecurityPolicy : defaultPolicy : 00-k0s-privileged workerProfiles : [] images : konnectivity : image : us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version : v0.0.13 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 kubeproxy : image : k8s.gcr.io/kube-proxy version : v1.19.0 coredns : image : docker.io/coredns/coredns version : 1.7.0 calico : cni : image : calico/cni version : v3.16.2 flexvolume : image : calico/pod2daemon-flexvol version : v3.16.2 node : image : calico/node version : v3.16.2 kubecontrollers : image : calico/kube-controllers version : v3.16.2 repository : \"\" telemetry : interval : 10m0s enabled : true extensions : helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : |2 values: \"for overriding\" namespace : default spec.api # address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes api-server process spec.controllerManager # extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes controller manager process spec.scheduler # extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes scheduler process spec.storage # type : Type of the data store, either etcd or kine . etcd.peerAddress : Nodes address to be used for etcd cluster peering. kine.dataSource : kine datasource URL. Using type etcd will make k0s to create and manage an elastic etcd cluster within the controller nodes. spec.network # provider : Network provider, either calico or custom . In case of custom user can push any network provider. podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services. spec.network.calico mode : vxlan (default) or ipip vxlanPort : The UDP port to use for VXLAN (default 4789 ) vxlanVNI : The virtual network ID to use for VXLAN. (default: 4096 ) mtu : MTU to use for overlay network (default 1450 ) wireguard : enable wireguard based encryption (default false ). Your host system must be wireguard ready. See https://docs.projectcalico.org/security/encrypt-cluster-pod-traffic for details. spec.podSecurityPolicy # Configures the default psp to be set. k0s creates two PSPs out of box: 00-k0s-privileged (default): no restrictions, always also used for Kubernetes/k0s level system pods 99-k0s-restricted : no host namespaces or root users allowed, no bind mounts from host As a user you can of course create any supplemental PSPs and bind them to users / access accounts as you need. spec.workerProfiles # Array of spec.workerProfiles.workerProfile Each element has following properties: - name : string, name, used as profile selector for the worker process - values : mapping object For each profile the control plane will create separate ConfigMap with kubelet-config yaml. Based on the --profile argument given to the k0s worker the corresponding ConfigMap would be used to extract kubelet-config.yaml from. values are recursively merged with default kubelet-config.yaml There are a few fields that cannot be overridden: - clusterDNS - clusterDomain - apiVersion - kind Example: workerProfiles: - name: custom-role values: key: value mapping: innerKey: innerValue images # Each node under the images key has the same structure images: konnectivity: image: calico/kube-controllers version: v3.16.2 Following keys are avaiable images.konnectivity images.metricsserver images.kubeproxy images.coredns images.calico.cni images.calico.flexvolume images.calico.node images.calico.kubecontrollers images.repository # If images.repository is set and not empty, every image name will be prefixed with the value of images.repository Example images: repository: \"my.own.repo\" konnectivity: image: calico/kube-controllers version: v3.16.2 In the runtime the image name will be calculated as my.own.repo/calico/kube-controllers:v3.16.2 . This only affects the location where images are getting pulled, omitting an image specification here will not disable the component from being deployed. Extensions # As stated in the project scope we intent to keep the scope of k0s quite small and not build gazillions of extensions into the product itself. To run k0s easily with your preferred extensions you have two options. Dump all needed extension manifest under /var/lib/k0s/manifests/my-extension . Read more on this approach here . Define your extensions as Helm charts : extensions: helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: |2 values: \"for overriding\" namespace: default This way you get a declarative way to configure the cluster and k0s controller manages the setup of the defined extension Helm charts as part of the cluster bootstrap process. Some examples what you could use as extension charts: - Ingress controllers: Nginx ingress , Traefix ingress ( tutorial ), - Volume storage providers: OpenEBS , Rook , Longhorn - Monitoring: Prometheus , Grafana Telemetry # To build better end user experience we collect and send telemetry data from clusters. It is enabled by default and can be disabled by settings corresponding option as false The default interval is 10 minutes, any valid value for time.Duration string representation can be used as a value. Example telemetry: interval: 2m0s enabled: true","title":"Configuration"},{"location":"configuration/#k0s-configuration","text":"","title":"k0s configuration"},{"location":"configuration/#control-plane","text":"k0s Control plane can be configured via a YAML config file. By default k0s server command reads a file called k0s.yaml but can be told to read any yaml file via --config option. An example config file with the most common options users should configure: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.106 sans : - my-k0s-control.my-domain.com network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 extensions : helm : repositories : - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" namespace : default","title":"Control plane"},{"location":"configuration/#specapi","text":"address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate","title":"spec.api"},{"location":"configuration/#specnetwork","text":"podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services.","title":"spec.network"},{"location":"configuration/#extensionshelm","text":"List of Helm repositories and charts to deploy during cluster bootstrap. This example configures Prometheus from \"stable\" Helms chart repository.","title":"extensions.helm"},{"location":"configuration/#configuring-multi-node-controlplane","text":"When configuring an elastic/HA controlplane one must use same configuration options on each node for the cluster level options. Following options need to match on each node, otherwise the control plane components will end up in very unknown states: - network - storage : Needless to say, one cannot create a clustered controlplane with each node only storing data locally on SQLite.","title":"Configuring multi-node controlplane"},{"location":"configuration/#full-config-reference","text":"Note: Many of the options configure things deep down in the \"stack\" on various components. So please make sure you understand what is being configured and whether or not it works in your specific environment. A full config file with defaults generated by the k0s default-config command: apiVersion : k0s.k0sproject.io/v1beta1 kind : Cluster metadata : name : k0s spec : api : address : 192.168.68.106 sans : - 192.168.68.106 - 192.168.68.106 extraArgs : {} controllerManager : extraArgs : {} scheduler : extraArgs : {} storage : type : etcd etcd : peerAddress : 192.168.68.106 network : podCIDR : 10.244.0.0/16 serviceCIDR : 10.96.0.0/12 provider : calico calico : mode : vxlan vxlanPort : 4789 vxlanVNI : 4096 mtu : 1450 wireguard : false podSecurityPolicy : defaultPolicy : 00-k0s-privileged workerProfiles : [] images : konnectivity : image : us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version : v0.0.13 metricsserver : image : gcr.io/k8s-staging-metrics-server/metrics-server version : v0.3.7 kubeproxy : image : k8s.gcr.io/kube-proxy version : v1.19.0 coredns : image : docker.io/coredns/coredns version : 1.7.0 calico : cni : image : calico/cni version : v3.16.2 flexvolume : image : calico/pod2daemon-flexvol version : v3.16.2 node : image : calico/node version : v3.16.2 kubecontrollers : image : calico/kube-controllers version : v3.16.2 repository : \"\" telemetry : interval : 10m0s enabled : true extensions : helm : repositories : - name : stable url : https://charts.helm.sh/stable - name : prometheus-community url : https://prometheus-community.github.io/helm-charts charts : - name : prometheus-stack chartname : prometheus-community/prometheus version : \"11.16.8\" values : |2 values: \"for overriding\" namespace : default","title":"Full config reference"},{"location":"configuration/#specapi_1","text":"address : The local address to bind API on. Also used as one of the addresses pushed on the k0s create service certificate on the API. Defaults to first non-local address found on the node. sans : List of additional addresses to push to API servers serving certificate extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes api-server process","title":"spec.api"},{"location":"configuration/#speccontrollermanager","text":"extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes controller manager process","title":"spec.controllerManager"},{"location":"configuration/#specscheduler","text":"extraArgs : Map of key-values (strings) for any extra arguments you wish to pass down to Kubernetes scheduler process","title":"spec.scheduler"},{"location":"configuration/#specstorage","text":"type : Type of the data store, either etcd or kine . etcd.peerAddress : Nodes address to be used for etcd cluster peering. kine.dataSource : kine datasource URL. Using type etcd will make k0s to create and manage an elastic etcd cluster within the controller nodes.","title":"spec.storage"},{"location":"configuration/#specnetwork_1","text":"provider : Network provider, either calico or custom . In case of custom user can push any network provider. podCIDR : Pod network CIDR to be used in the cluster serviceCIDR : Network CIDR to be used for cluster VIP services.","title":"spec.network"},{"location":"configuration/#specpodsecuritypolicy","text":"Configures the default psp to be set. k0s creates two PSPs out of box: 00-k0s-privileged (default): no restrictions, always also used for Kubernetes/k0s level system pods 99-k0s-restricted : no host namespaces or root users allowed, no bind mounts from host As a user you can of course create any supplemental PSPs and bind them to users / access accounts as you need.","title":"spec.podSecurityPolicy"},{"location":"configuration/#specworkerprofiles","text":"Array of spec.workerProfiles.workerProfile Each element has following properties: - name : string, name, used as profile selector for the worker process - values : mapping object For each profile the control plane will create separate ConfigMap with kubelet-config yaml. Based on the --profile argument given to the k0s worker the corresponding ConfigMap would be used to extract kubelet-config.yaml from. values are recursively merged with default kubelet-config.yaml There are a few fields that cannot be overridden: - clusterDNS - clusterDomain - apiVersion - kind Example: workerProfiles: - name: custom-role values: key: value mapping: innerKey: innerValue","title":"spec.workerProfiles"},{"location":"configuration/#images","text":"Each node under the images key has the same structure images: konnectivity: image: calico/kube-controllers version: v3.16.2 Following keys are avaiable","title":"images"},{"location":"configuration/#imagesrepository","text":"If images.repository is set and not empty, every image name will be prefixed with the value of images.repository Example images: repository: \"my.own.repo\" konnectivity: image: calico/kube-controllers version: v3.16.2 In the runtime the image name will be calculated as my.own.repo/calico/kube-controllers:v3.16.2 . This only affects the location where images are getting pulled, omitting an image specification here will not disable the component from being deployed.","title":"images.repository"},{"location":"configuration/#extensions","text":"As stated in the project scope we intent to keep the scope of k0s quite small and not build gazillions of extensions into the product itself. To run k0s easily with your preferred extensions you have two options. Dump all needed extension manifest under /var/lib/k0s/manifests/my-extension . Read more on this approach here . Define your extensions as Helm charts : extensions: helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: |2 values: \"for overriding\" namespace: default This way you get a declarative way to configure the cluster and k0s controller manages the setup of the defined extension Helm charts as part of the cluster bootstrap process. Some examples what you could use as extension charts: - Ingress controllers: Nginx ingress , Traefix ingress ( tutorial ), - Volume storage providers: OpenEBS , Rook , Longhorn - Monitoring: Prometheus , Grafana","title":"Extensions"},{"location":"configuration/#telemetry","text":"To build better end user experience we collect and send telemetry data from clusters. It is enabled by default and can be disabled by settings corresponding option as false The default interval is 10 minutes, any valid value for time.Duration string representation can be used as a value. Example telemetry: interval: 2m0s enabled: true","title":"Telemetry"},{"location":"conformance-testing/","text":"Kubernetes conformance testing for k0s # We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository. In a nutshell, you need to: - Setup k0s on some VMs/bare metal boxes - Download, if you do not already have, sonobuoy tool - Run the conformance tests with something like sonobuoy run --mode=certified-conformance - Wait for couple hours - Collect results","title":"Kubernetes conformance testing for k0s"},{"location":"conformance-testing/#kubernetes-conformance-testing-for-k0s","text":"We run the conformance testing for the last RC build for a release. Follow the instructions as the conformance testing repository. In a nutshell, you need to: - Setup k0s on some VMs/bare metal boxes - Download, if you do not already have, sonobuoy tool - Run the conformance tests with something like sonobuoy run --mode=certified-conformance - Wait for couple hours - Collect results","title":"Kubernetes conformance testing for k0s"},{"location":"containerd_config/","text":"containerd configuration # containerd is industry-standard container runtime. NOTE: In most use cases changes to the containerd configuration will not be required. In order to make changes to containerd configuration first you need to create default containerd configuration by running: containerd config default > /etc/k0s/containerd.toml This command will dump default values to /etc/k0s/containerd.toml . k0s runs containerd with follwoing default values: /var/lib/k0s/bin/containerd \\ --root=/var/lib/k0s/containerd \\ --state=/var/lib/k0s/run/containerd \\ --address=/var/lib/k0s/run/containerd.sock \\ --config=/etc/k0s/containerd.toml Before proceeding further make sure that following default values are added to the configuration file: version = 2 root = \"/var/lib/k0s/containerd\" state = \"/var/lib/k0s/run/containerd\" ... [grpc] address = \"/var/lib/k0s/run/containerd.sock\" Next if you want to change CRI look into this section [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\" Using gVisor # gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system. First you must install the needed gVisor binaries into the host. ( set -e URL = https://storage.googleapis.com/gvisor/releases/release/latest wget ${ URL } /runsc ${ URL } /runsc.sha512 \\ ${ URL } /gvisor-containerd-shim ${ URL } /gvisor-containerd-shim.sha512 \\ ${ URL } /containerd-shim-runsc-v1 ${ URL } /containerd-shim-runsc-v1.sha512 sha512sum -c runsc.sha512 \\ -c gvisor-containerd-shim.sha512 \\ -c containerd-shim-runsc-v1.sha512 rm -f *.sha512 chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1 sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin ) See gVisor install docs Next we need to prepare the config for k0s managed containerD to utilize gVisor as additional runtime: cat <<EOF | sudo tee /etc/k0s/containerd.toml disabled_plugins = [\"restart\"] [plugins.linux] shim_debug = true [plugins.cri.containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF Then we can start and join the worker as normally into the cluster: k0s worker $token By default containerd uses nromal runc as the runtime. To make gVisor runtime usable for workloads we must register it to Kubernetes side: cat <<EOF | kubectl apply -f - apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: gvisor handler: runsc EOF After this we can use it for our workloads: apiVersion : v1 kind : Pod metadata : name : nginx-gvisor spec : runtimeClassName : gvisor containers : - name : nginx image : nginx We can verify the created nginx pod is actually running under gVisor runtime: # kubectl exec nginx-gvisor -- dmesg | grep -i gvisor [ 0.000000] Starting gVisor... Using custom nvidia-container-runtime # By default CRI is set tu runC and if you want to configure Nvidia GPU support you will have to replace runc with nvidia-container-runtime as shown below: [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"nvidia-container-runtime\" Note To run nvidia-container-runtime on your node please look here for detailed instructions. After changes to the configuration, restart k0s and in this case containerd will be using newly configured runtime.","title":"containerd configuration"},{"location":"containerd_config/#containerd-configuration","text":"containerd is industry-standard container runtime. NOTE: In most use cases changes to the containerd configuration will not be required. In order to make changes to containerd configuration first you need to create default containerd configuration by running: containerd config default > /etc/k0s/containerd.toml This command will dump default values to /etc/k0s/containerd.toml . k0s runs containerd with follwoing default values: /var/lib/k0s/bin/containerd \\ --root=/var/lib/k0s/containerd \\ --state=/var/lib/k0s/run/containerd \\ --address=/var/lib/k0s/run/containerd.sock \\ --config=/etc/k0s/containerd.toml Before proceeding further make sure that following default values are added to the configuration file: version = 2 root = \"/var/lib/k0s/containerd\" state = \"/var/lib/k0s/run/containerd\" ... [grpc] address = \"/var/lib/k0s/run/containerd.sock\" Next if you want to change CRI look into this section [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\"","title":"containerd configuration"},{"location":"containerd_config/#using-gvisor","text":"gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system. First you must install the needed gVisor binaries into the host. ( set -e URL = https://storage.googleapis.com/gvisor/releases/release/latest wget ${ URL } /runsc ${ URL } /runsc.sha512 \\ ${ URL } /gvisor-containerd-shim ${ URL } /gvisor-containerd-shim.sha512 \\ ${ URL } /containerd-shim-runsc-v1 ${ URL } /containerd-shim-runsc-v1.sha512 sha512sum -c runsc.sha512 \\ -c gvisor-containerd-shim.sha512 \\ -c containerd-shim-runsc-v1.sha512 rm -f *.sha512 chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1 sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin ) See gVisor install docs Next we need to prepare the config for k0s managed containerD to utilize gVisor as additional runtime: cat <<EOF | sudo tee /etc/k0s/containerd.toml disabled_plugins = [\"restart\"] [plugins.linux] shim_debug = true [plugins.cri.containerd.runtimes.runsc] runtime_type = \"io.containerd.runsc.v1\" EOF Then we can start and join the worker as normally into the cluster: k0s worker $token By default containerd uses nromal runc as the runtime. To make gVisor runtime usable for workloads we must register it to Kubernetes side: cat <<EOF | kubectl apply -f - apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata: name: gvisor handler: runsc EOF After this we can use it for our workloads: apiVersion : v1 kind : Pod metadata : name : nginx-gvisor spec : runtimeClassName : gvisor containers : - name : nginx image : nginx We can verify the created nginx pod is actually running under gVisor runtime: # kubectl exec nginx-gvisor -- dmesg | grep -i gvisor [ 0.000000] Starting gVisor...","title":"Using gVisor"},{"location":"containerd_config/#using-custom-nvidia-container-runtime","text":"By default CRI is set tu runC and if you want to configure Nvidia GPU support you will have to replace runc with nvidia-container-runtime as shown below: [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"nvidia-container-runtime\" Note To run nvidia-container-runtime on your node please look here for detailed instructions. After changes to the configuration, restart k0s and in this case containerd will be using newly configured runtime.","title":"Using custom nvidia-container-runtime"},{"location":"create-cluster/","text":"Creating A cluster with k0s # As k0s binary has everything it needs packaged into a single binary, it makes it super easy to spin up Kubernetes clusters. Pre-requisites # Download k0s binary from releases and push it to all the nodes you wish to connect to the cluster. That's it, really. Bootstrapping controller node # Create a configuration file if you wish to tune some of the settings. $ k0s server -c k0s.yaml That's it, really. k0s process will act as a \"supervisor\" for all the control plane components. In few seconds you'll have the control plane up-and-running. Naturally, to make k0s boot up the control plane when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system. Create join token # To be able to join workers into the cluster we need a token. The token embeds information with which we can enable mutual trust between the worker and controller(s) and allow the node to join the cluster as worker. To get a token run the following on one of the existing controller nodes: k0s token create --role = worker This will output a long token which we will use to join the worker. To enhance security, we can also set an expiration time on the tokens by using: k0s token create --role = worker --expiry = \"100h\" Joining worker(s) to cluster # To join the worker we need to run k0s in worker mode with the token from previous step: $ k0s worker \"long-join-token\" That's it, really. Naturally, to make k0s boot up the worker components when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system. Tokens # The tokens are actually base64 encoded kubeconfigs . Why: - well defined structure - can be used directly as bootstrap auth configs for kubelet - embeds CA info for mutual trust The actual bearer token embedded in the kubeconfig is a bootstrap token . For controller join token and for worker join token we use different usage attributes so we can make sure we can validate the token role on the controller side. Join controller node # To be able to join a new controller node into the cluster you must be using either etcd or some externalized data store (MySQL or Postgres) via kine. Also make sure the configurations match for the data storage on all controller nodes. To create a join token for the new controller, run the following on existing controller node: k0s token create --role = controller --expiry = 1h The on the new controller, run: k0s server \"long-join-token\" Adding a Cluster User # To add a user to cluster, use the user create command. This will output a kubeconfig for the user, which can be used for authentication. On the controller, run the following to generate a kubeconfig for a user : shell script k0s user create [username] ### Enabling Access to Cluster Resources To allow the user access to the cluster, the user needs to be created with the `system:masters` group: ```shell script clusterUser=\"testUser\" k0s user create --groups \"system:masters\" $clusterUser > ~/clusterUser.kubeconfig Create the proper roleBinding, to allow the user access to the resources: shell script kubectl create clusterrolebinding $clusterUser-admin-binding --clusterrole=admin --user=$clusterUser ```","title":"Quick Start Guide"},{"location":"create-cluster/#creating-a-cluster-with-k0s","text":"As k0s binary has everything it needs packaged into a single binary, it makes it super easy to spin up Kubernetes clusters.","title":"Creating A cluster with k0s"},{"location":"create-cluster/#pre-requisites","text":"Download k0s binary from releases and push it to all the nodes you wish to connect to the cluster. That's it, really.","title":"Pre-requisites"},{"location":"create-cluster/#bootstrapping-controller-node","text":"Create a configuration file if you wish to tune some of the settings. $ k0s server -c k0s.yaml That's it, really. k0s process will act as a \"supervisor\" for all the control plane components. In few seconds you'll have the control plane up-and-running. Naturally, to make k0s boot up the control plane when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system.","title":"Bootstrapping controller node"},{"location":"create-cluster/#create-join-token","text":"To be able to join workers into the cluster we need a token. The token embeds information with which we can enable mutual trust between the worker and controller(s) and allow the node to join the cluster as worker. To get a token run the following on one of the existing controller nodes: k0s token create --role = worker This will output a long token which we will use to join the worker. To enhance security, we can also set an expiration time on the tokens by using: k0s token create --role = worker --expiry = \"100h\"","title":"Create join token"},{"location":"create-cluster/#joining-workers-to-cluster","text":"To join the worker we need to run k0s in worker mode with the token from previous step: $ k0s worker \"long-join-token\" That's it, really. Naturally, to make k0s boot up the worker components when the node itself reboots you should really make the k0s process to be supervised by systemd or some other init system.","title":"Joining worker(s) to cluster"},{"location":"create-cluster/#tokens","text":"The tokens are actually base64 encoded kubeconfigs . Why: - well defined structure - can be used directly as bootstrap auth configs for kubelet - embeds CA info for mutual trust The actual bearer token embedded in the kubeconfig is a bootstrap token . For controller join token and for worker join token we use different usage attributes so we can make sure we can validate the token role on the controller side.","title":"Tokens"},{"location":"create-cluster/#join-controller-node","text":"To be able to join a new controller node into the cluster you must be using either etcd or some externalized data store (MySQL or Postgres) via kine. Also make sure the configurations match for the data storage on all controller nodes. To create a join token for the new controller, run the following on existing controller node: k0s token create --role = controller --expiry = 1h The on the new controller, run: k0s server \"long-join-token\"","title":"Join controller node"},{"location":"create-cluster/#adding-a-cluster-user","text":"To add a user to cluster, use the user create command. This will output a kubeconfig for the user, which can be used for authentication. On the controller, run the following to generate a kubeconfig for a user : shell script k0s user create [username] ### Enabling Access to Cluster Resources To allow the user access to the cluster, the user needs to be created with the `system:masters` group: ```shell script clusterUser=\"testUser\" k0s user create --groups \"system:masters\" $clusterUser > ~/clusterUser.kubeconfig Create the proper roleBinding, to allow the user access to the resources: shell script kubectl create clusterrolebinding $clusterUser-admin-binding --clusterrole=admin --user=$clusterUser ```","title":"Adding a Cluster User"},{"location":"custom-cri-runtime/","text":"Custom CRI runtime # k0s supports users bringing their own CRI runtime such as Docker. In this case k0s will not start nor manage the runtime, it's fully up to the user to configure it properly. To run k0s worker with a custom CRI runtime use the option --cri-socket . It takes input in the form of <type>:<socket> where: - type : Either remote or docker . Use docker for pure docker setup, remote for everything else. - socket : Path to the socket, examples: unix:///var/run/docker.sock To run k0s with pre-existing docker setup run the worker with k0s worker --cri-socket docker:unix:///var/run/docker.sock <token> . In case docker is used k0s will configure kubelet to create the dockershim socket at /var/run/dockershim.sock .","title":"Custom CRI runtime"},{"location":"custom-cri-runtime/#custom-cri-runtime","text":"k0s supports users bringing their own CRI runtime such as Docker. In this case k0s will not start nor manage the runtime, it's fully up to the user to configure it properly. To run k0s worker with a custom CRI runtime use the option --cri-socket . It takes input in the form of <type>:<socket> where: - type : Either remote or docker . Use docker for pure docker setup, remote for everything else. - socket : Path to the socket, examples: unix:///var/run/docker.sock To run k0s with pre-existing docker setup run the worker with k0s worker --cri-socket docker:unix:///var/run/docker.sock <token> . In case docker is used k0s will configure kubelet to create the dockershim socket at /var/run/dockershim.sock .","title":"Custom CRI runtime"},{"location":"k0s-in-docker/","text":"k0s in Docker # We publish k0s container image for every release. By default we run both controller and worker in the same container to allow easy local test \"cluster\". You can run your own k0s-in-docker easily with: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.pkg.github.com/k0sproject/k0s/k0s:<version> Just grab the kubeconfig with docker exec k0s cat /var/lib/k0s/pki/admin.conf and paste e.g. into Lens . Running workers # If you want to attach multiple workers nodes into the cluster you can run separate containers for workers. First, we need a join token for the worker: token = $( docker exec -t -i k0s k0s token create --role = worker ) Then join a new worker by running the container with: docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.pkg.github.com/k0sproject/k0s/k0s:<version> k0s worker $token Repeat for as many workers you need, and have resources for. :) Known limitations # No custom Docker networks # Currently we cannot run k0s nodes if the containers are configured to use custom networks e.g. with --net my-net . This is caused by the fact that Docker sets up a custom DNS service within the network and that messes up CoreDNS. We know that there are some workarounds possible, but they are bit hackish. And on the other hand, running k0s cluster(s) in bridge network should not cause issues.","title":"Run in Docker"},{"location":"k0s-in-docker/#k0s-in-docker","text":"We publish k0s container image for every release. By default we run both controller and worker in the same container to allow easy local test \"cluster\". You can run your own k0s-in-docker easily with: docker run -d --name k0s --hostname k0s --privileged -v /var/lib/k0s -p 6443 :6443 docker.pkg.github.com/k0sproject/k0s/k0s:<version> Just grab the kubeconfig with docker exec k0s cat /var/lib/k0s/pki/admin.conf and paste e.g. into Lens .","title":"k0s in Docker"},{"location":"k0s-in-docker/#running-workers","text":"If you want to attach multiple workers nodes into the cluster you can run separate containers for workers. First, we need a join token for the worker: token = $( docker exec -t -i k0s k0s token create --role = worker ) Then join a new worker by running the container with: docker run -d --name k0s-worker1 --hostname k0s-worker1 --privileged -v /var/lib/k0s docker.pkg.github.com/k0sproject/k0s/k0s:<version> k0s worker $token Repeat for as many workers you need, and have resources for. :)","title":"Running workers"},{"location":"k0s-in-docker/#known-limitations","text":"","title":"Known limitations"},{"location":"k0s-in-docker/#no-custom-docker-networks","text":"Currently we cannot run k0s nodes if the containers are configured to use custom networks e.g. with --net my-net . This is caused by the fact that Docker sets up a custom DNS service within the network and that messes up CoreDNS. We know that there are some workarounds possible, but they are bit hackish. And on the other hand, running k0s cluster(s) in bridge network should not cause issues.","title":"No custom Docker networks"},{"location":"k0s-single-node/","text":"k0s Single Node Quick Start # This outlines a quick method to running local k0s master and worker in one node. 1) Prepare Dependencies 1. Download the k0s binary sudo curl --output /usr/local/sbin/k0s -L https://github.com/k0sproject/k0s/releases/download/v0.7.0/k0s-v0.7.0-amd64 2. Download the kubectl binary sudo curl --output /usr/local/sbin/kubectl -L \"https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl\" 3. Make both binaries executeable sudo chmod +x /usr/local/sbin/ { kubectl,k0s } 2) Start k0s 1. Create ~/.k0s directory mkdir -p ${ HOME } /.k0s 2. Generate default k0s.yaml cluster configuration k0s default-config | tee ${ HOME } /.k0s/k0s.yaml 3. Start k0s sudo k0s server -c ${ HOME } /.k0s/k0s.yaml --enable-worker & 3) Monitor Startup k0s 1. Save kubeconfig for user sudo cat /var/lib/k0s/pki/admin.conf | tee ~/.k0s/kubeconfig 2. Set the KUBECONFIG environment variable export KUBECONFIG = \" ${ HOME } /.k0s/kubeconfig\" 3. Monitor cluster startup kubectl get pods --all-namespaces","title":"Single-node set up"},{"location":"k0s-single-node/#k0s-single-node-quick-start","text":"This outlines a quick method to running local k0s master and worker in one node. 1) Prepare Dependencies 1. Download the k0s binary sudo curl --output /usr/local/sbin/k0s -L https://github.com/k0sproject/k0s/releases/download/v0.7.0/k0s-v0.7.0-amd64 2. Download the kubectl binary sudo curl --output /usr/local/sbin/kubectl -L \"https://storage.googleapis.com/kubernetes-release/release/ $( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt ) /bin/linux/amd64/kubectl\" 3. Make both binaries executeable sudo chmod +x /usr/local/sbin/ { kubectl,k0s } 2) Start k0s 1. Create ~/.k0s directory mkdir -p ${ HOME } /.k0s 2. Generate default k0s.yaml cluster configuration k0s default-config | tee ${ HOME } /.k0s/k0s.yaml 3. Start k0s sudo k0s server -c ${ HOME } /.k0s/k0s.yaml --enable-worker & 3) Monitor Startup k0s 1. Save kubeconfig for user sudo cat /var/lib/k0s/pki/admin.conf | tee ~/.k0s/kubeconfig 2. Set the KUBECONFIG environment variable export KUBECONFIG = \" ${ HOME } /.k0s/kubeconfig\" 3. Monitor cluster startup kubectl get pods --all-namespaces","title":"k0s Single Node Quick Start"},{"location":"manifests/","text":"Manifest deployer # k0s embeds a manifest deployer on controllers which allows easy way to deploy manifests automatically. By default k0s reads all manifests in /var/lib/k0s/manifests and ensures their state matches on the cluster. When you remove a manifest file k0s will automatically prune all the resources associated with it. Each directory that is a direct descendant of /var/lib/k0s/manifests is considered to be its own stack, but nested directories are not considered new stacks. Note: k0s uses this mechanism for some of it's internal in-cluster components and other resources. Make sure you only touch the manifests not managed by k0s. Future # We may in the future support nested directories, but those will not be considered stacks , but rather subresources of a parent stack. Stacks are exclusively top-level.","title":"Manifest deployer"},{"location":"manifests/#manifest-deployer","text":"k0s embeds a manifest deployer on controllers which allows easy way to deploy manifests automatically. By default k0s reads all manifests in /var/lib/k0s/manifests and ensures their state matches on the cluster. When you remove a manifest file k0s will automatically prune all the resources associated with it. Each directory that is a direct descendant of /var/lib/k0s/manifests is considered to be its own stack, but nested directories are not considered new stacks. Note: k0s uses this mechanism for some of it's internal in-cluster components and other resources. Make sure you only touch the manifests not managed by k0s.","title":"Manifest deployer"},{"location":"manifests/#future","text":"We may in the future support nested directories, but those will not be considered stacks , but rather subresources of a parent stack. Stacks are exclusively top-level.","title":"Future"},{"location":"network/","text":"k0s Networking # In-cluster networking # k0s supports currently only Calico as the built-in in-cluster overlay network provider. A user can however opt-out of k0s managing the network setup by using a custom as the network type. Using custom network provider it is expected that the user sets up the networking. This can be achieved e.g. by pushing network provider manifests into /var/lib/k0s/manifests from where k0s controllers will pick them up and deploy into the cluster. More on the automatic manifest handling here . Controller(s) - Worker communication # As one of the goals of k0s is to allow deployment of totally isolated control plane we cannot rely on the fact that there is an IP route between controller nodes and the pod overlay network. To enable this communication path, which is mandated by conformance tests, we use Egress service and konnectivity proxy to proxy the traffic from API server into worker nodes. This ansures that we can always fulfill all the Kubernetes API functionalities but still operate the control plane in total isolation from the workers. Needed open ports & protocols # Protocol Port Service Direction Notes TCP 2380 etcd peers controller <-> controller TCP 6443 kube-apiserver Worker, CLI => controller authenticated kube API using kube TLS client certs, ServiceAccount tokens with RBAC UDP 4789 Calico worker <-> worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker => Host * authenticated kubelet API for the master node kube-apiserver (and heapster / metrics-server addons) using TLS client certs TCP 9443 k0s-api controller <-> controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker <-> controller konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets","title":"k0s Networking"},{"location":"network/#k0s-networking","text":"","title":"k0s Networking"},{"location":"network/#in-cluster-networking","text":"k0s supports currently only Calico as the built-in in-cluster overlay network provider. A user can however opt-out of k0s managing the network setup by using a custom as the network type. Using custom network provider it is expected that the user sets up the networking. This can be achieved e.g. by pushing network provider manifests into /var/lib/k0s/manifests from where k0s controllers will pick them up and deploy into the cluster. More on the automatic manifest handling here .","title":"In-cluster networking"},{"location":"network/#controllers-worker-communication","text":"As one of the goals of k0s is to allow deployment of totally isolated control plane we cannot rely on the fact that there is an IP route between controller nodes and the pod overlay network. To enable this communication path, which is mandated by conformance tests, we use Egress service and konnectivity proxy to proxy the traffic from API server into worker nodes. This ansures that we can always fulfill all the Kubernetes API functionalities but still operate the control plane in total isolation from the workers.","title":"Controller(s) - Worker communication"},{"location":"network/#needed-open-ports-protocols","text":"Protocol Port Service Direction Notes TCP 2380 etcd peers controller <-> controller TCP 6443 kube-apiserver Worker, CLI => controller authenticated kube API using kube TLS client certs, ServiceAccount tokens with RBAC UDP 4789 Calico worker <-> worker Calico VXLAN overlay TCP 10250 kubelet Master, Worker => Host * authenticated kubelet API for the master node kube-apiserver (and heapster / metrics-server addons) using TLS client certs TCP 9443 k0s-api controller <-> controller k0s controller join API, TLS with token auth TCP 8132,8133 konnectivity server worker <-> controller konnectivity is used as \"reverse\" tunnel between kube-apiserver and worker kubelets","title":"Needed open ports &amp; protocols"},{"location":"troubleshooting/","text":"Troubleshooting # There are few common cases we've seen where k0s fails to run properly. CoreDNS in crashloop # The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s). With kubectl you see something like this: $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-25px6 1 /1 Running 0 167m kube-system calico-node-fwjx5 1 /1 Running 0 164m kube-system calico-node-t4tx5 1 /1 Running 0 164m kube-system calico-node-whwsg 1 /1 Running 0 164m kube-system coredns-5c98d7d4d8-tfs4q 1 /1 Error 17 167m kube-system konnectivity-agent-9jkfd 1 /1 Running 0 164m kube-system konnectivity-agent-bvhdb 1 /1 Running 0 164m kube-system konnectivity-agent-r6mzj 1 /1 Running 0 164m kube-system kube-proxy-kr2r9 1 /1 Running 0 164m kube-system kube-proxy-tbljr 1 /1 Running 0 164m kube-system kube-proxy-xbw7p 1 /1 Running 0 164m kube-system metrics-server-7d4bcb75dd-pqkrs 1 /1 Running 0 167m When you check the logs, it'll show something like this: $ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q plugin/loop: Loop (127.0.0.1:55953 -> :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\" This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries. The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts /etc/resolv.conf to original Read more at CoreDNS troubleshooting docs . k0s server fails on ARM boxes # In the logs you probably see ETCD not starting up properly. Etcd is not fully supported on ARM architecture, thus you need to run k0s server and thus also etcd process with env ETCD_UNSUPPORTED_ARCH=arm64 . As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either. Pods pending when using cloud providers # Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint node.cloudprovider.kubernetes.io/uninitialized for the node. This tain will prevent normal workloads to be scheduled on the node untill the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not schedulable untill the cloud provider controller is actually succesfully running on the cluster. For troubleshooting your specific cloud provider see its documentation.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"There are few common cases we've seen where k0s fails to run properly.","title":"Troubleshooting"},{"location":"troubleshooting/#coredns-in-crashloop","text":"The most common case we've encountered so far has been CoreDNS getting into crashloop on the node(s). With kubectl you see something like this: $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5f6546844f-25px6 1 /1 Running 0 167m kube-system calico-node-fwjx5 1 /1 Running 0 164m kube-system calico-node-t4tx5 1 /1 Running 0 164m kube-system calico-node-whwsg 1 /1 Running 0 164m kube-system coredns-5c98d7d4d8-tfs4q 1 /1 Error 17 167m kube-system konnectivity-agent-9jkfd 1 /1 Running 0 164m kube-system konnectivity-agent-bvhdb 1 /1 Running 0 164m kube-system konnectivity-agent-r6mzj 1 /1 Running 0 164m kube-system kube-proxy-kr2r9 1 /1 Running 0 164m kube-system kube-proxy-tbljr 1 /1 Running 0 164m kube-system kube-proxy-xbw7p 1 /1 Running 0 164m kube-system metrics-server-7d4bcb75dd-pqkrs 1 /1 Running 0 167m When you check the logs, it'll show something like this: $ kubectl -n kube-system logs coredns-5c98d7d4d8-tfs4q plugin/loop: Loop (127.0.0.1:55953 -> :1053) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 4547991504243258144.3688648895315093531.\" This is most often caused by systemd-resolved stub (or something similar) running locally and CoreDNS detects a possible loop with DNS queries. The easiest but most crude way to workaround is to disable the systemd-resolved stub and revert the hosts /etc/resolv.conf to original Read more at CoreDNS troubleshooting docs .","title":"CoreDNS in crashloop"},{"location":"troubleshooting/#k0s-server-fails-on-arm-boxes","text":"In the logs you probably see ETCD not starting up properly. Etcd is not fully supported on ARM architecture, thus you need to run k0s server and thus also etcd process with env ETCD_UNSUPPORTED_ARCH=arm64 . As Etcd is not fully supported on ARM architecture it also means that k0s controlplane with etcd itself is not fully supported on ARM either.","title":"k0s server fails on ARM boxes"},{"location":"troubleshooting/#pods-pending-when-using-cloud-providers","text":"Once we enable cloud provider support on kubelet on worker nodes, kubelet will automatically add a taint node.cloudprovider.kubernetes.io/uninitialized for the node. This tain will prevent normal workloads to be scheduled on the node untill the cloud provider controller actually runs second initialization on the node and removes the taint. This means that these nodes are not schedulable untill the cloud provider controller is actually succesfully running on the cluster. For troubleshooting your specific cloud provider see its documentation.","title":"Pods pending when using cloud providers"},{"location":"cli/","text":"k0s # k0s - Zero Friction Kubernetes Synopsis # k0s is yet another Kubernetes distro. Yes. But we do some of the things pretty different from other distros out there. It is a single binary all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it. Options # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1]) SEE ALSO # k0s api - Run the controller api k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s server - Run server k0s token - Manage join tokens k0s user - Manage user access k0s version - Print the k0s version k0s worker - Run worker Auto generated by spf13/cobra on 24-Nov-2020","title":"Index"},{"location":"cli/#k0s","text":"k0s - Zero Friction Kubernetes","title":"k0s"},{"location":"cli/#synopsis","text":"k0s is yet another Kubernetes distro. Yes. But we do some of the things pretty different from other distros out there. It is a single binary all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it.","title":"Synopsis"},{"location":"cli/#options","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1])","title":"Options"},{"location":"cli/#see-also","text":"k0s api - Run the controller api k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s server - Run server k0s token - Manage join tokens k0s user - Manage user access k0s version - Print the k0s version k0s worker - Run worker","title":"SEE ALSO"},{"location":"cli/k0s/","text":"k0s # k0s - Zero Friction Kubernetes Synopsis # k0s is yet another Kubernetes distro. Yes. But we do some of the things pretty different from other distros out there. It is a single binary all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it. Options # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1]) SEE ALSO # k0s api - Run the controller api k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s server - Run server k0s token - Manage join tokens k0s user - Manage user access k0s version - Print the k0s version k0s worker - Run worker Auto generated by spf13/cobra on 24-Nov-2020","title":"K0s"},{"location":"cli/k0s/#k0s","text":"k0s - Zero Friction Kubernetes","title":"k0s"},{"location":"cli/k0s/#synopsis","text":"k0s is yet another Kubernetes distro. Yes. But we do some of the things pretty different from other distros out there. It is a single binary all-inclusive Kubernetes distribution with all the required bells and whistles preconfigured to make building a Kubernetes clusters a matter of just copying an executable to every host and running it.","title":"Synopsis"},{"location":"cli/k0s/#options","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -h, --help help for k0s -l, --logging stringToString Logging Levels for the different components (default [kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1])","title":"Options"},{"location":"cli/k0s/#see-also","text":"k0s api - Run the controller api k0s default-config - Output the default k0s configuration yaml to stdout k0s docs - Generate Markdown docs for the k0s binary k0s etcd - Manage etcd cluster k0s server - Run server k0s token - Manage join tokens k0s user - Manage user access k0s version - Print the k0s version k0s worker - Run worker","title":"SEE ALSO"},{"location":"cli/k0s_api/","text":"k0s api # Run the controller api k0s api [flags] Options # -h, --help help for api Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s api"},{"location":"cli/k0s_api/#k0s-api","text":"Run the controller api k0s api [flags]","title":"k0s api"},{"location":"cli/k0s_api/#options","text":"-h, --help help for api","title":"Options"},{"location":"cli/k0s_api/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_api/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_default-config/","text":"k0s default-config # Output the default k0s configuration yaml to stdout k0s default-config [flags] Options # -h, --help help for default-config Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s default config"},{"location":"cli/k0s_default-config/#k0s-default-config","text":"Output the default k0s configuration yaml to stdout k0s default-config [flags]","title":"k0s default-config"},{"location":"cli/k0s_default-config/#options","text":"-h, --help help for default-config","title":"Options"},{"location":"cli/k0s_default-config/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_default-config/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_docs/","text":"k0s docs # Generate Markdown docs for the k0s binary k0s docs [flags] Options # -h, --help help for docs Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s docs"},{"location":"cli/k0s_docs/#k0s-docs","text":"Generate Markdown docs for the k0s binary k0s docs [flags]","title":"k0s docs"},{"location":"cli/k0s_docs/#options","text":"-h, --help help for docs","title":"Options"},{"location":"cli/k0s_docs/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_docs/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_etcd/","text":"k0s etcd # Manage etcd cluster Options # -h, --help help for etcd Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s etcd leave - Sign off a given etc node from etcd cluster k0s etcd member-list - Returns etcd cluster members list Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s etcd"},{"location":"cli/k0s_etcd/#k0s-etcd","text":"Manage etcd cluster","title":"k0s etcd"},{"location":"cli/k0s_etcd/#options","text":"-h, --help help for etcd","title":"Options"},{"location":"cli/k0s_etcd/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s etcd leave - Sign off a given etc node from etcd cluster k0s etcd member-list - Returns etcd cluster members list","title":"SEE ALSO"},{"location":"cli/k0s_etcd_leave/","text":"k0s etcd leave # Sign off a given etc node from etcd cluster k0s etcd leave [flags] Options # -h, --help help for leave --peer-address string etcd peer address Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s etcd - Manage etcd cluster Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s etcd leave"},{"location":"cli/k0s_etcd_leave/#k0s-etcd-leave","text":"Sign off a given etc node from etcd cluster k0s etcd leave [flags]","title":"k0s etcd leave"},{"location":"cli/k0s_etcd_leave/#options","text":"-h, --help help for leave --peer-address string etcd peer address","title":"Options"},{"location":"cli/k0s_etcd_leave/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd_leave/#see-also","text":"k0s etcd - Manage etcd cluster","title":"SEE ALSO"},{"location":"cli/k0s_etcd_member-list/","text":"k0s etcd member-list # Returns etcd cluster members list k0s etcd member-list [flags] Options # -h, --help help for member-list Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s etcd - Manage etcd cluster Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s etcd member list"},{"location":"cli/k0s_etcd_member-list/#k0s-etcd-member-list","text":"Returns etcd cluster members list k0s etcd member-list [flags]","title":"k0s etcd member-list"},{"location":"cli/k0s_etcd_member-list/#options","text":"-h, --help help for member-list","title":"Options"},{"location":"cli/k0s_etcd_member-list/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_etcd_member-list/#see-also","text":"k0s etcd - Manage etcd cluster","title":"SEE ALSO"},{"location":"cli/k0s_server/","text":"k0s server # Run server k0s server [join-token] [flags] Examples # Command to associate master nodes: CLI argument: $ k0s server [join-token] or CLI flag: $ k0s server --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag Options # --enable-worker enable worker (default false) -h, --help help for server --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing join-token. Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s server"},{"location":"cli/k0s_server/#k0s-server","text":"Run server k0s server [join-token] [flags]","title":"k0s server"},{"location":"cli/k0s_server/#examples","text":"Command to associate master nodes: CLI argument: $ k0s server [join-token] or CLI flag: $ k0s server --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag","title":"Examples"},{"location":"cli/k0s_server/#options","text":"--enable-worker enable worker (default false) -h, --help help for server --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing join-token.","title":"Options"},{"location":"cli/k0s_server/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_server/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_token/","text":"k0s token # Manage join tokens k0s token [flags] Options # -h, --help help for token --kubeconfig string path to kubeconfig file [$KUBECONFIG] (default \"/var/lib/k0s/pki/admin.conf\") Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s token create - Create join token Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s token"},{"location":"cli/k0s_token/#k0s-token","text":"Manage join tokens k0s token [flags]","title":"k0s token"},{"location":"cli/k0s_token/#options","text":"-h, --help help for token --kubeconfig string path to kubeconfig file [$KUBECONFIG] (default \"/var/lib/k0s/pki/admin.conf\")","title":"Options"},{"location":"cli/k0s_token/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_token/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s token create - Create join token","title":"SEE ALSO"},{"location":"cli/k0s_token_create/","text":"k0s token create # Create join token k0s token create [flags] Options # --expiry string set duration time for token (default \"0\") -h, --help help for create --role string Either worker or controller (default \"worker\") --wait wait forever (default false) Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s token - Manage join tokens Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s token create"},{"location":"cli/k0s_token_create/#k0s-token-create","text":"Create join token k0s token create [flags]","title":"k0s token create"},{"location":"cli/k0s_token_create/#options","text":"--expiry string set duration time for token (default \"0\") -h, --help help for create --role string Either worker or controller (default \"worker\") --wait wait forever (default false)","title":"Options"},{"location":"cli/k0s_token_create/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_token_create/#see-also","text":"k0s token - Manage join tokens","title":"SEE ALSO"},{"location":"cli/k0s_user/","text":"k0s user # Manage user access k0s user [flags] Options # -h, --help help for user Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes k0s user create - Create a kubeconfig for a user Auto generated by spf13/cobra on 24-Nov-2020","title":"K0s user"},{"location":"cli/k0s_user/#k0s-user","text":"Manage user access k0s user [flags]","title":"k0s user"},{"location":"cli/k0s_user/#options","text":"-h, --help help for user","title":"Options"},{"location":"cli/k0s_user/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_user/#see-also","text":"k0s - k0s - Zero Friction Kubernetes k0s user create - Create a kubeconfig for a user","title":"SEE ALSO"},{"location":"cli/k0s_user_create/","text":"k0s user create # Create a kubeconfig for a user Synopsis # Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user k0s user create [username] [flags] Examples # Command to create a kubeconfig for a user: CLI argument: $ k0s user create [username] optionally add groups: $ k0s user create [username] --groups [groups] Options # --groups string Specify groups -h, --help help for create Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1]) SEE ALSO # k0s user - Manage user access Auto generated by spf13/cobra on 24-Nov-2020","title":"K0s user create"},{"location":"cli/k0s_user_create/#k0s-user-create","text":"Create a kubeconfig for a user","title":"k0s user create"},{"location":"cli/k0s_user_create/#synopsis","text":"Create a kubeconfig with a signed certificate and public key for a given user (and optionally user groups) Note: A certificate once signed cannot be revoked for a particular user k0s user create [username] [flags]","title":"Synopsis"},{"location":"cli/k0s_user_create/#examples","text":"Command to create a kubeconfig for a user: CLI argument: $ k0s user create [username] optionally add groups: $ k0s user create [username] --groups [groups]","title":"Examples"},{"location":"cli/k0s_user_create/#options","text":"--groups string Specify groups -h, --help help for create","title":"Options"},{"location":"cli/k0s_user_create/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [kube-scheduler=1,kubelet=1,etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_user_create/#see-also","text":"k0s user - Manage user access","title":"SEE ALSO"},{"location":"cli/k0s_version/","text":"k0s version # Print the k0s version k0s version [flags] Options # -h, --help help for version Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s version"},{"location":"cli/k0s_version/#k0s-version","text":"Print the k0s version k0s version [flags]","title":"k0s version"},{"location":"cli/k0s_version/#options","text":"-h, --help help for version","title":"Options"},{"location":"cli/k0s_version/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_version/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"cli/k0s_worker/","text":"k0s worker # Run worker k0s worker [join-token] [flags] Examples # Command to add worker node to the master node: CLI argument: $ k0s worker [token] or CLI flag: $ k0s worker --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag Options # --cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token. Options inherited from parent commands # -c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1]) SEE ALSO # k0s - k0s - Zero Friction Kubernetes Auto generated by spf13/cobra on 20-Nov-2020","title":"K0s worker"},{"location":"cli/k0s_worker/#k0s-worker","text":"Run worker k0s worker [join-token] [flags]","title":"k0s worker"},{"location":"cli/k0s_worker/#examples","text":"Command to add worker node to the master node: CLI argument: $ k0s worker [token] or CLI flag: $ k0s worker --token-file [path_to_file] Note: Token can be passed either as a CLI argument or as a flag","title":"Examples"},{"location":"cli/k0s_worker/#options","text":"--cri-socket string contrainer runtime socket to use, default to internal containerd. Format: [remote|docker]:[path-to-socket] --enable-cloud-provider Whether or not to enable cloud provider support in kubelet -h, --help help for worker --profile string worker profile to use on the node (default \"default\") --token-file string Path to the file containing token.","title":"Options"},{"location":"cli/k0s_worker/#options-inherited-from-parent-commands","text":"-c, --config string config file (default: ./k0s.yaml) --data-dir string Data Directory for k0s (default: /var/lib/k0s). DO NOT CHANGE for an existing setup, things will break! -d, --debug Debug logging (default: false) -l, --logging stringToString Logging Levels for the different components (default [etcd=info,containerd=info,konnectivity-server=1,kube-apiserver=1,kube-controller-manager=1,kube-scheduler=1,kubelet=1])","title":"Options inherited from parent commands"},{"location":"cli/k0s_worker/#see-also","text":"k0s - k0s - Zero Friction Kubernetes","title":"SEE ALSO"},{"location":"contributors/github_workflow/","text":"Github Workflow # Fork The Project Adding the Forked Remote Create & Rebase Your Feature Branch Commit & Push Open a Pull Request Get a code review Squash commits Push Your Final Changes This guide assumes you have already cloned the upstream repo to your system via git clone, or via go get github.com/k0sproject/k0s . Fork The Project # Go to http://github.com/k0sproject/k0s On the top, right-hand side, click on \"fork\" and select your username for the fork destination. Adding the Forked Remote # export GITHUB_USER={ your github's username } cd $WORKDIR/k0s git remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git # Prevent push to Upstream git remote set-url --push origin no_push # Set your fork remote as a default push target git push --set-upstream $GITHUB_USER main Your remotes should look something like this: \u279c git remote -v origin https://github.com/k0sproject/k0s (fetch) origin no_push (push) my_fork git@github.com:{ github_username }/k0s.git (fetch) my_fork git@github.com:{ github_username }/k0s.git (push) Create & Rebase Your Feature Branch # Create a feature branch: git branch -b my_feature_branch Rebase your branch: git fetch origin git rebase origin/main Current branch my_feature_branch is up to date. Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful. Commit & Push # Commit and sign your changes: git commit -m \"my commit title\" --signoff You can go back and edit/build/test some more, then commit --amend in a few cycles. When ready, push your changes to your fork's repository: git push --set-upstream my_fork my_feature_branch Open a Pull Request # Github Docs Get a code review # Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests. Commit changes made in response to review comments should be added to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review. Squashing Commits # Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed. To do that, it's best to perform an interactive rebase : Example If you PR has 3 commits, count backwards from your last commit using HEAD~3 : git rebase -i HEAD~3 Output would be similar to this: pick f7f3f6d Changed some code pick 310154e fixed some typos pick a5f4a0d made some review changes # Rebase 710f0f8..a5f4a0d onto 710f0f8 # # Commands: # p, pick <commit> = use commit # r, reword <commit> = use commit, but edit the commit message # e, edit <commit> = use commit, but stop for amending # s, squash <commit> = use commit, but meld into previous commit # f, fixup <commit> = like \"squash\", but discard this commit's log message # x, exec <command> = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with 'git rebase --continue') # d, drop <commit> = remove commit # l, label <label> = label current HEAD with a name # t, reset <label> = reset HEAD to a label # m, merge [-C <commit> | -c <commit>] <label> [# <oneline>] # . create a merge commit using the original merge commit's # . message (or the oneline, if no original merge commit was # . specified). Use -c <commit> to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out Use a command line text editor to change the word pick to fixup for the commits you want to squash, then save your changes and continue the rebase: Per the output above, you can see that: fixup <commit> = like \"squash\", but discard this commit's log message Which means that when rebased, the commit message \"fixed some typos\" will be removed, and squashed with the parent commit. Push Your Final Changes # Once done, you can push the final commits to your branch: git push --force You can run multiple iteration of rebase / push -f , if needed.","title":"Workflow"},{"location":"contributors/github_workflow/#github-workflow","text":"Fork The Project Adding the Forked Remote Create & Rebase Your Feature Branch Commit & Push Open a Pull Request Get a code review Squash commits Push Your Final Changes This guide assumes you have already cloned the upstream repo to your system via git clone, or via go get github.com/k0sproject/k0s .","title":"Github Workflow"},{"location":"contributors/github_workflow/#fork-the-project","text":"Go to http://github.com/k0sproject/k0s On the top, right-hand side, click on \"fork\" and select your username for the fork destination.","title":"Fork The Project"},{"location":"contributors/github_workflow/#adding-the-forked-remote","text":"export GITHUB_USER={ your github's username } cd $WORKDIR/k0s git remote add $GITHUB_USER git@github.com:${GITHUB_USER}/k0s.git # Prevent push to Upstream git remote set-url --push origin no_push # Set your fork remote as a default push target git push --set-upstream $GITHUB_USER main Your remotes should look something like this: \u279c git remote -v origin https://github.com/k0sproject/k0s (fetch) origin no_push (push) my_fork git@github.com:{ github_username }/k0s.git (fetch) my_fork git@github.com:{ github_username }/k0s.git (push)","title":"Adding the Forked Remote"},{"location":"contributors/github_workflow/#create-rebase-your-feature-branch","text":"Create a feature branch: git branch -b my_feature_branch Rebase your branch: git fetch origin git rebase origin/main Current branch my_feature_branch is up to date. Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful.","title":"Create &amp; Rebase Your Feature Branch"},{"location":"contributors/github_workflow/#commit-push","text":"Commit and sign your changes: git commit -m \"my commit title\" --signoff You can go back and edit/build/test some more, then commit --amend in a few cycles. When ready, push your changes to your fork's repository: git push --set-upstream my_fork my_feature_branch","title":"Commit &amp; Push"},{"location":"contributors/github_workflow/#open-a-pull-request","text":"Github Docs","title":"Open a Pull Request"},{"location":"contributors/github_workflow/#get-a-code-review","text":"Once your pull request has been opened it will be assigned to one or more reviewers, and will go through a series of smoke tests. Commit changes made in response to review comments should be added to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review.","title":"Get a code review"},{"location":"contributors/github_workflow/#squashing-commits","text":"Commits on your branch should represent meaningful milestones or units of work. Small commits that contain typo fixes, rebases, review feedbacks, etc should be squashed. To do that, it's best to perform an interactive rebase :","title":"Squashing Commits"},{"location":"contributors/github_workflow/#push-your-final-changes","text":"Once done, you can push the final commits to your branch: git push --force You can run multiple iteration of rebase / push -f , if needed.","title":"Push Your Final Changes"},{"location":"contributors/testing/","text":"Testing Your Code # k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR. Run Local Verifications # Please run the following style and formatting commands and fix/check-in any changes: 1. Linting We use golangci-lint for style verification. In the repository's root directory, simply run: make lint 2. Go fmt go fmt ./... 3. Pre-submit Flight Checks In the repository root directory, make sure that: make build runs successfully. make check-basic runs successfully. make check-unit has no errors. make check-hacontrolplane runs successfully. Please note that this last test is prone to \"flakiness\", so it might fail on occasion. If it fails constantly, take a deeper look at your code to find the source of the problem. If you find that all tests passed, you may open a pull request upstream. Opening A Pull Request # Draft Mode You may open a pull request in draft mode . All automated tests will still run against the PR, but the PR will not be assigned for review. Once a PR is ready for review, transition it from Draft mode, and code owners will be notified. Conformance Testing Once a PR has been reviewed and all other tests have passed, a code owner will run a full end-to-end conformance test against the PR. This is usually the last step before merging. Pre-Requisites for PR Merge In order for a PR to be merged, the following conditions should exist: 1. The PR has passed all the automated tests (style, build & conformance tests). 2. PR commits have been signed with the --signoff option. 3. PR was reviewed and approved by a code owner. 4. PR is rebased against upstream's main branch.","title":"Testing"},{"location":"contributors/testing/#testing-your-code","text":"k0s uses github actions to run automated tests on any PR, before merging. However, a PR will not be reviewed before all tests are green, so to save time and prevent your PR from going stale, it is best to test it before submitting the PR.","title":"Testing Your Code"},{"location":"contributors/testing/#run-local-verifications","text":"Please run the following style and formatting commands and fix/check-in any changes:","title":"Run Local Verifications"},{"location":"contributors/testing/#opening-a-pull-request","text":"","title":"Opening A Pull Request"},{"location":"examples/traefik-ingress/","text":"Installing the Traefik Ingress Controller on k0s # In this tutorial, you'll learn how to configure k0s with the Traefik ingress controller , a MetalLB service loadbalancer , and deploy the Traefik Dashboard along with a service example. Utilizing the extensible bootstrapping functionality with Helm, it's as simple as adding the right extensions to the k0s.yaml file when configuring your cluster. Configuring k0s.yaml # Modify your k0s.yaml file to include the Traefik and MetalLB helm charts as extensions, and these will install during the cluster's bootstrap. Note: You may want to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool allocated by your DHCP server. Providing an addressable range should allow you to access your LoadBalancer and Ingress services from anywhere on your local network. However, any valid IP range should work locally on your machine. extensions : helm : repositories : - name : traefik url : https://helm.traefik.io/traefik - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : traefik chartname : traefik/traefik version : \"9.11.0\" namespace : default - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 192.168.0.5-192.168.0.10 Providing a range of IPs for MetalLB that are addressable on your LAN is suggested if you want to access LoadBalancer and Ingress services from anywhere on your local network. Retrieving the Load Balancer IP # Once you've started your cluster, you should confirm the deployment of Traefik and MetalLB. Executing a kubectl get all should include a response with the metallb and traefik resources, along with a service loadbalancer that has an EXTERNAL-IP assigned to it. See the example below: root@k0s-host \u279c kubectl get all NAME READY STATUS RESTARTS AGE pod/metallb-1607085578-controller-864c9757f6-bpx6r 1 /1 Running 0 81s pod/metallb-1607085578-speaker-245c2 1 /1 Running 0 60s pod/traefik-1607085579-77bbc57699-b2f2t 1 /1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 96s service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/metallb-1607085578-speaker 1 1 1 1 1 kubernetes.io/os = linux 87s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-1607085578-controller 1 /1 1 1 87s deployment.apps/traefik-1607085579 1 /1 1 1 84s NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-1607085578-controller-864c9757f6 1 1 1 81s replicaset.apps/traefik-1607085579-77bbc57699 1 1 1 81s Take note of the EXTERNAL-IP given to the service/traefik-n LoadBalancer. In this example, 192.168.0.5 has been assigned and can be used to access services via the Ingress proxy: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s # Recieving a 404 response here is normal, as you've not configured any Ingress resources to respond yet root@k0s-host \u279c curl http://192.168.0.5 404 page not found Deploy and access the Traefik Dashboard # Now that you have an available and addressable load balancer on your cluster, you can quickly deploy the Traefik dashboard and access it from anywhere on your local network (provided that you configured MetalLB with an addressable range). Create the Traefik Dashboard IngressRoute in a YAML file: apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : dashboard spec : entryPoints : - web routes : - match : PathPrefix(`/dashboard`) || PathPrefix(`/api`) kind : Rule services : - name : api@internal kind : TraefikService Next, deploy the resource: root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml ingressroute.traefik.containo.us/dashboard created Once deployed, you should be able to access the dashboard using the EXTERNAL-IP that you noted above by visiting http://192.168.0.5 in your browser: Now, create a simple whoami Deployment, Service, and Ingress manifest: apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami-container image : containous/whoami --- apiVersion : v1 kind : Service metadata : name : whoami-service spec : ports : - name : http targetPort : 80 port : 80 selector : app : whoami --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : whoami-ingress spec : rules : - http : paths : - path : /whoami pathType : Exact backend : service : name : whoami-service port : number : 80 Once you've created this, apply and test it: # apply the manifests root@k0s-host \u279c kubectl apply -f whoami.yaml deployment.apps/whoami-deployment created service/whoami-service created ingress.networking.k8s.io/whoami-ingress created # test the ingress and service root@k0s-host \u279c curl http://192.168.0.5/whoami Hostname: whoami-deployment-85bfbd48f-7l77c IP: 127 .0.0.1 IP: ::1 IP: 10 .244.214.198 IP: fe80::b049:f8ff:fe77:3e64 RemoteAddr: 10 .244.214.196:34858 GET /whoami HTTP/1.1 Host: 192 .168.0.5 User-Agent: curl/7.68.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 192 .168.0.82 X-Forwarded-Host: 192 .168.0.5 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t X-Real-Ip: 192 .168.0.82 Summary # From here, it's possible to use 3rd party tools, such as ngrok , to go further and expose your LoadBalancer to the world. Doing so then enables dynamic certificate provisioning through Let's Encrypt utilizing either cert-manager or Traefik's own built-in ACME provider . This guide should have given you a general idea of getting started with Ingress on k0s and exposing your applications and services quickly.","title":"Installing the Traefik Ingress Controller on k0s"},{"location":"examples/traefik-ingress/#installing-the-traefik-ingress-controller-on-k0s","text":"In this tutorial, you'll learn how to configure k0s with the Traefik ingress controller , a MetalLB service loadbalancer , and deploy the Traefik Dashboard along with a service example. Utilizing the extensible bootstrapping functionality with Helm, it's as simple as adding the right extensions to the k0s.yaml file when configuring your cluster.","title":"Installing the Traefik Ingress Controller on k0s"},{"location":"examples/traefik-ingress/#configuring-k0syaml","text":"Modify your k0s.yaml file to include the Traefik and MetalLB helm charts as extensions, and these will install during the cluster's bootstrap. Note: You may want to have a small range of IP addresses that are addressable on your network, preferably outside the assignment pool allocated by your DHCP server. Providing an addressable range should allow you to access your LoadBalancer and Ingress services from anywhere on your local network. However, any valid IP range should work locally on your machine. extensions : helm : repositories : - name : traefik url : https://helm.traefik.io/traefik - name : bitnami url : https://charts.bitnami.com/bitnami charts : - name : traefik chartname : traefik/traefik version : \"9.11.0\" namespace : default - name : metallb chartname : bitnami/metallb version : \"1.0.1\" namespace : default values : |2 configInline: address-pools: - name: generic-cluster-pool protocol: layer2 addresses: - 192.168.0.5-192.168.0.10 Providing a range of IPs for MetalLB that are addressable on your LAN is suggested if you want to access LoadBalancer and Ingress services from anywhere on your local network.","title":"Configuring k0s.yaml"},{"location":"examples/traefik-ingress/#retrieving-the-load-balancer-ip","text":"Once you've started your cluster, you should confirm the deployment of Traefik and MetalLB. Executing a kubectl get all should include a response with the metallb and traefik resources, along with a service loadbalancer that has an EXTERNAL-IP assigned to it. See the example below: root@k0s-host \u279c kubectl get all NAME READY STATUS RESTARTS AGE pod/metallb-1607085578-controller-864c9757f6-bpx6r 1 /1 Running 0 81s pod/metallb-1607085578-speaker-245c2 1 /1 Running 0 60s pod/traefik-1607085579-77bbc57699-b2f2t 1 /1 Running 0 81s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 96s service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/metallb-1607085578-speaker 1 1 1 1 1 kubernetes.io/os = linux 87s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-1607085578-controller 1 /1 1 1 87s deployment.apps/traefik-1607085579 1 /1 1 1 84s NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-1607085578-controller-864c9757f6 1 1 1 81s replicaset.apps/traefik-1607085579-77bbc57699 1 1 1 81s Take note of the EXTERNAL-IP given to the service/traefik-n LoadBalancer. In this example, 192.168.0.5 has been assigned and can be used to access services via the Ingress proxy: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/traefik-1607085579 LoadBalancer 10 .105.119.102 192 .168.0.5 80 :32153/TCP,443:30791/TCP 84s # Recieving a 404 response here is normal, as you've not configured any Ingress resources to respond yet root@k0s-host \u279c curl http://192.168.0.5 404 page not found","title":"Retrieving the Load Balancer IP"},{"location":"examples/traefik-ingress/#deploy-and-access-the-traefik-dashboard","text":"Now that you have an available and addressable load balancer on your cluster, you can quickly deploy the Traefik dashboard and access it from anywhere on your local network (provided that you configured MetalLB with an addressable range). Create the Traefik Dashboard IngressRoute in a YAML file: apiVersion : traefik.containo.us/v1alpha1 kind : IngressRoute metadata : name : dashboard spec : entryPoints : - web routes : - match : PathPrefix(`/dashboard`) || PathPrefix(`/api`) kind : Rule services : - name : api@internal kind : TraefikService Next, deploy the resource: root@k0s-host \u279c kubectl apply -f traefik-dashboard.yaml ingressroute.traefik.containo.us/dashboard created Once deployed, you should be able to access the dashboard using the EXTERNAL-IP that you noted above by visiting http://192.168.0.5 in your browser: Now, create a simple whoami Deployment, Service, and Ingress manifest: apiVersion : apps/v1 kind : Deployment metadata : name : whoami-deployment spec : replicas : 1 selector : matchLabels : app : whoami template : metadata : labels : app : whoami spec : containers : - name : whoami-container image : containous/whoami --- apiVersion : v1 kind : Service metadata : name : whoami-service spec : ports : - name : http targetPort : 80 port : 80 selector : app : whoami --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : whoami-ingress spec : rules : - http : paths : - path : /whoami pathType : Exact backend : service : name : whoami-service port : number : 80 Once you've created this, apply and test it: # apply the manifests root@k0s-host \u279c kubectl apply -f whoami.yaml deployment.apps/whoami-deployment created service/whoami-service created ingress.networking.k8s.io/whoami-ingress created # test the ingress and service root@k0s-host \u279c curl http://192.168.0.5/whoami Hostname: whoami-deployment-85bfbd48f-7l77c IP: 127 .0.0.1 IP: ::1 IP: 10 .244.214.198 IP: fe80::b049:f8ff:fe77:3e64 RemoteAddr: 10 .244.214.196:34858 GET /whoami HTTP/1.1 Host: 192 .168.0.5 User-Agent: curl/7.68.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 192 .168.0.82 X-Forwarded-Host: 192 .168.0.5 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: traefik-1607085579-77bbc57699-b2f2t X-Real-Ip: 192 .168.0.82","title":"Deploy and access the Traefik Dashboard"},{"location":"examples/traefik-ingress/#summary","text":"From here, it's possible to use 3rd party tools, such as ngrok , to go further and expose your LoadBalancer to the world. Doing so then enables dynamic certificate provisioning through Let's Encrypt utilizing either cert-manager or Traefik's own built-in ACME provider . This guide should have given you a general idea of getting started with Ingress on k0s and exposing your applications and services quickly.","title":"Summary"},{"location":"internal/extensions/","text":"Cluster extensions # k0s allows users to use extensions to extend cluster functionality. At the moment the only supported type of extensions is helm based charts. The default configuration has no extensions. Helm based extensions # Configuration # Example. helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: |2 <embed yaml> namespace: default By using the configuration above, the cluster would: - add stable and prometheus-community chart repositories - install the `prometheus-community/prometheus` chart of the specified version to the `default` namespace. The chart installation is implemented by using CRD `helm.k0sproject.io/Chart`. For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations: - install - upgrade - delete For security reasons, the cluster operates only on Chart CRDs instantiated in the `kube-system` namespace, however, the target namespace could be any. #### CRD definition apiVersion: helm.k0sproject.io/v1beta1 kind: Chart metadata: creationTimestamp: \"2020-11-10T14:17:53Z\" generation: 2 labels: k0s.k0sproject.io/stack: helm name: k0s-addon-chart-test-addon namespace: kube-system resourceVersion: \"627\" selfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon uid: ebe59ed4-1ff8-4d41-8e33-005b183651ed spec: chartName: prometheus-community/prometheus namespace: default values: |2 key: value version: 11.16.8 status: appVersion: 2.21.0 namespace: default releaseName: prometheus-1605017878 revision: 2 updated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901 version: 11.16.8 The Chart.spec defines the chart information. The Chart.status keeps the information about the last operation performed by the operator.","title":"Cluster extensions"},{"location":"internal/extensions/#cluster-extensions","text":"k0s allows users to use extensions to extend cluster functionality. At the moment the only supported type of extensions is helm based charts. The default configuration has no extensions.","title":"Cluster extensions"},{"location":"internal/extensions/#helm-based-extensions","text":"","title":"Helm based extensions"},{"location":"internal/extensions/#configuration","text":"Example. helm: repositories: - name: stable url: https://charts.helm.sh/stable - name: prometheus-community url: https://prometheus-community.github.io/helm-charts charts: - name: prometheus-stack chartname: prometheus-community/prometheus version: \"11.16.8\" values: |2 <embed yaml> namespace: default By using the configuration above, the cluster would: - add stable and prometheus-community chart repositories - install the `prometheus-community/prometheus` chart of the specified version to the `default` namespace. The chart installation is implemented by using CRD `helm.k0sproject.io/Chart`. For every given helm extension the cluster creates a Chart CRD instance. The cluster has a controller which monitors for the Chart CRDs, supporting the following operations: - install - upgrade - delete For security reasons, the cluster operates only on Chart CRDs instantiated in the `kube-system` namespace, however, the target namespace could be any. #### CRD definition apiVersion: helm.k0sproject.io/v1beta1 kind: Chart metadata: creationTimestamp: \"2020-11-10T14:17:53Z\" generation: 2 labels: k0s.k0sproject.io/stack: helm name: k0s-addon-chart-test-addon namespace: kube-system resourceVersion: \"627\" selfLink: /apis/helm.k0sproject.io/v1beta1/namespaces/kube-system/charts/k0s-addon-chart-test-addon uid: ebe59ed4-1ff8-4d41-8e33-005b183651ed spec: chartName: prometheus-community/prometheus namespace: default values: |2 key: value version: 11.16.8 status: appVersion: 2.21.0 namespace: default releaseName: prometheus-1605017878 revision: 2 updated: 2020-11-10 14:18:08.235656 +0000 UTC m=+41.871656901 version: 11.16.8 The Chart.spec defines the chart information. The Chart.status keeps the information about the last operation performed by the operator.","title":"Configuration"},{"location":"internal/host-dependencies/","text":"Host Dependencies # The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies. List of hard dependencies # find -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189 du -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that du dependency remains, but using POSIX-compliant argument nice iptables -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether iptables is needed. It appears to come from the portmap plugin, but the most robust solution may be to simply bundle iptables with k0s.","title":"Host Dependencies"},{"location":"internal/host-dependencies/#host-dependencies","text":"The goal of k0s is to only depend on kernel functionality. However, the kubelet currently has several host dependencies. Some of these may need to be bundled, but we should prefer upstream contributions to remove these dependencies.","title":"Host Dependencies"},{"location":"internal/host-dependencies/#list-of-hard-dependencies","text":"find -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95189 du -- PR by @ncopa to resolve this: https://github.com/kubernetes/kubernetes/pull/95178 -- note that du dependency remains, but using POSIX-compliant argument nice iptables -- as documented in https://github.com/k0sproject/k0s/issues/176 it is unclear whether iptables is needed. It appears to come from the portmap plugin, but the most robust solution may be to simply bundle iptables with k0s.","title":"List of hard dependencies"},{"location":"internal/upgrading-calico/","text":"Upgrading Calico # k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs . As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version: run ./get-calico.sh check the git diff to see if it looks sensible re-apply our manual adjustments (documented below) run make bindata-manifests compile, pray, and test commit and create a PR Manual Adjustments # Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications , not the calico originals. static/manifests/calico/DaemonSet/calico-node.yaml : variable-based support for both vxlan and ipip (search for ipip to find): {{- if eq .Mode \"ipip\" }} # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Always\" # Enable or Disable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Never\" {{- else if eq .Mode \"vxlan\" }} # Disable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Never\" # Enable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Always\" - name: FELIX_VXLANPORT value: \"{{ .VxlanPort }}\" - name: FELIX_VXLANVNI value: \"{{ .VxlanVNI }}\" {{- end }} variable-based WireGuard support: {{- if .EnableWireguard }} - name: FELIX_WIREGUARDENABLED value: \"true\" {{- end }} variable-based cluster CIDR: - name: CALICO_IPV4POOL_CIDR value: \"{{ .ClusterCIDR }}\" custom backend and MTU # calico-config.yaml calico_backend: \"{{ .Mode }}\" veth_mtu: \"{{ .MTU }}\" remove bgp from CLUSTER_TYPE - name: CLUSTER_TYPE value: \"k8s\" disable BIRD checks on liveness and readiness as we don't support BGP by removing -bird-ready and -bird-live from the readiness and liveness probes respectively Container image names # Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used: CalicoCNIImage for calico/cni CalicoFlexVolumeImage for calico/pod2daemon-flexvol CalicoNodeImage for calico/node CalicoKubeControllersImage for calico/kube-controllers Example: # calico-node.yaml image: {{ .CalicoCNIImage }}","title":"Upgrading Calico"},{"location":"internal/upgrading-calico/#upgrading-calico","text":"k0s bundles Kubernetes manifests for Calico. The manifests are retrieved from the official Calico docs . As fetching and modifying the entire multi-thousand line file is error-prone, you may follow these steps to upgrade Calico to the latest version: run ./get-calico.sh check the git diff to see if it looks sensible re-apply our manual adjustments (documented below) run make bindata-manifests compile, pray, and test commit and create a PR","title":"Upgrading Calico"},{"location":"internal/upgrading-calico/#manual-adjustments","text":"Note: All manual adjustments should be fairly obvious from the git diff. This section attempts to provide a sanity checklist to go through and make sure we still have those changes applied. The code blocks in this section are our modifications , not the calico originals. static/manifests/calico/DaemonSet/calico-node.yaml : variable-based support for both vxlan and ipip (search for ipip to find): {{- if eq .Mode \"ipip\" }} # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Always\" # Enable or Disable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Never\" {{- else if eq .Mode \"vxlan\" }} # Disable IPIP - name: CALICO_IPV4POOL_IPIP value: \"Never\" # Enable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \"Always\" - name: FELIX_VXLANPORT value: \"{{ .VxlanPort }}\" - name: FELIX_VXLANVNI value: \"{{ .VxlanVNI }}\" {{- end }} variable-based WireGuard support: {{- if .EnableWireguard }} - name: FELIX_WIREGUARDENABLED value: \"true\" {{- end }} variable-based cluster CIDR: - name: CALICO_IPV4POOL_CIDR value: \"{{ .ClusterCIDR }}\" custom backend and MTU # calico-config.yaml calico_backend: \"{{ .Mode }}\" veth_mtu: \"{{ .MTU }}\" remove bgp from CLUSTER_TYPE - name: CLUSTER_TYPE value: \"k8s\" disable BIRD checks on liveness and readiness as we don't support BGP by removing -bird-ready and -bird-live from the readiness and liveness probes respectively","title":"Manual Adjustments"},{"location":"internal/upgrading-calico/#container-image-names","text":"Instead of hardcoded image names and versions use placeholders to support configuration level settings. Following placeholders are used: CalicoCNIImage for calico/cni CalicoFlexVolumeImage for calico/pod2daemon-flexvol CalicoNodeImage for calico/node CalicoKubeControllersImage for calico/kube-controllers Example: # calico-node.yaml image: {{ .CalicoCNIImage }}","title":"Container image names"}]}